{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 9\n",
    "---\n",
    "Hello All,\n",
    "\n",
    "Welcome to the DSCI 510's lab.\n",
    "\n",
    "Guidelines for the Lab:\n",
    "- You will be given the lab assignment in the start of the lab. You're supposed to complete it by the deadline. \n",
    "\n",
    "- You've to complete the assignments individually. If you are having trouble completing the assignment do let me know, I will clear your doubts and guide you but I'll not write code for you and no one else should :) !!!  \n",
    "\n",
    "- You have to fill in the code in this notebook and upload it to back to blackboard for submission. While doing this, make sure that all supporting files that you download from blackboard are in the same directory as this notebook.  \n",
    "\n",
    "- You are encouraged to look up resources online like python docs and stackoverflow. But, you are encouraged to look up the topics and not the questions themselves  \n",
    "\n",
    "- Your last submission will be counted towards your grade  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's lab is about BeautifulSoup and requests.  \n",
    "\n",
    "Some hints before you go: \n",
    "- Use the ```type``` function and ```dir``` function whenever possible to understand what you have vs what did you expect and what you can do with what you have.\n",
    "- Give attention to the brackets while looking at a json so that you know what you will get after parsing it. \n",
    "- Similarly, give attention to the tags while looking at html code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q1.[15 points] \n",
    "---\n",
    "\n",
    "For this question, we will scrape the website \"https://www.dailynews.com/\".  \n",
    "\n",
    "For all news headlines for the website, you have to look for the link to the article and the headline text which **contains the keyword(specifically, in the headline) passed to you in the function argument**.  \n",
    "\n",
    "For the return values, you have to return a list of tuples where a particular tuple's 0th value is the link to the article and the 1st value is the headline text.   \n",
    "\n",
    "Note 1: Keyword and headlines should not be considered case-sensitive.\n",
    "Note 2: Call ```strip()``` over the link and the headline text, so that it is more readable.  \n",
    "\n",
    "FAQs:  \n",
    "Q. How to read/parse a website?  \n",
    "A. For that, as taught in the class you would need to use libraries like BeautifulSoup, urllib or requests.  \n",
    "\n",
    "Q. What am I getting anyway when I read the website?  \n",
    "A. You are getting the whole html code written for the website, when you read/parse the website.  \n",
    "\n",
    "Q. I don't know html, what should I do?  \n",
    "A. No worries. For this lab, you only need to understand that html is another type of a language made up of tags. For example, it has heading info in the *head* tag, paragraph has *p* tag, and html links have *a* tag. So, using the tag, you can get the info you need!  \n",
    "\n",
    "Q. How do I get the tag info?  \n",
    "A. When you get your BeautifulSoup object you can do something like object.find_all('tag') or object('tag') to get all 'tag' tags.  \n",
    "\n",
    "Q. What are tag attributes and how can I access that?  \n",
    "A. HTML attributes are special words used inside the tag to control the element's behaviour. One can access attributes using *get* function.\n",
    "\n",
    "Q. I got the tag, but what I got seems cryptic. What now?  \n",
    "A. So, once you get the info from a particular tag, you get the whole html code inside that tag. If you want some particular information from that, you would need to call functions like get and get_text to get the information directly without messing with the html code.  \n",
    "\n",
    "Q. I am not sure how to use these functions. Help?  \n",
    "A. You can read from the official documentation. Link: https://www.crummy.com/software/BeautifulSoup/bs4/doc/  \n",
    "\n",
    "Good Feature: Use the .prettify() feature of BeautifulSoup. You'll get a better insight of what is that you are scrapping. \n",
    "\n",
    "---\n",
    "\n",
    "Grading Rubric: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\truon\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\truon\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=9b449d7c24750bd011a88f5bd037ad650954bdb158659caa8b0dc3787b1b17d9\n",
      "  Stored in directory: c:\\users\\truon\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# Uncomment following line to install beautifulsoup if you find out that you don't have it\n",
    "!pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_news_of(keyword):\n",
    "    content = requests.get('https://www.dailynews.com/')\n",
    "    soup = BeautifulSoup(content.content, 'html.parser')\n",
    "    '''\n",
    "    Add your code below. \n",
    "    The variable soup defined above is what you will use to fetch the information\n",
    "    '''\n",
    "\n",
    "    mainTable = soup.findAll('a', {\"class\" : \"article-title\"})\n",
    "    list_site = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in mainTable:\n",
    "        t = i.get(\"title\")\n",
    "        h = i.get(\"href\")\n",
    "#with the assumption that this is not case sensitive\n",
    "        if keyword.lower() in t.lower() or keyword.lower() in h.lower():#if not then if keyword. in t or keyword in h:\n",
    "            list_site.append((t,h))\n",
    "            count += 1\n",
    "    print (count)\n",
    "    return list_site\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('South LA school thought to be first in county to reclose campus because of coronavirus',\n",
       "  'https://www.dailynews.com/2021/10/21/south-la-school-thought-to-be-first-in-county-to-reclose-campus-because-of-coronavirus/'),\n",
       " ('About 1,500 LAUSD employees granted accommodations for coronavirus vaccine exemptions',\n",
       "  'https://www.dailynews.com/2021/10/20/about-1500-lausd-employees-granted-accommodations-for-coronavirus-vaccine-exemptions/'),\n",
       " ('Coronavirus: L.A. County reported 1,267 new cases and 31 more deaths, Oct. 20',\n",
       "  'https://www.dailynews.com/2021/10/20/coronavirus-l-a-county-reported-1267-new-cases-and-31-more-deaths-oct-20/')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_news_of('Snyder')\n",
    "get_news_of('Coronavirus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris Taylor’s 3 home runs power Dodgers to Game 5 win over Braves',\n",
       "  'https://www.dailynews.com/2021/10/21/chris-taylors-3-home-runs-power-dodgers-to-game-5-win-over-braves/'),\n",
       " ('NLCS Game 5: Braves at Dodgers, lineups, starting pitchers, TV info',\n",
       "  'https://www.dailynews.com/2021/10/21/nlcs-game-5-braves-at-dodgers-lineups-starting-pitchers-tv-info/'),\n",
       " ('Whicker: Dodgers’ Albert Pujols parachutes back into October, finds it familiar',\n",
       "  'https://www.dailynews.com/2021/10/21/whicker-dodgers-albert-pujols-parachutes-back-into-october-finds-it-familiar/'),\n",
       " ('Photos: Eddie Rosario, Braves blast Dodgers in NLCS Game 4',\n",
       "  'https://www.dailynews.com/2021/10/20/photos-dodgers-vs-braves-in-nlcs-game-4-2/'),\n",
       " ('Photos: Unforgettable images as Dodgers win NLCS Game 3',\n",
       "  'https://www.dailynews.com/2021/10/19/photos-nlcs-game-3-dodgers-vs-braves/'),\n",
       " ('Dodgers lose to Braves on another walk-off single, trail NLCS 2-0',\n",
       "  'https://www.dailynews.com/2021/10/17/dodgers-lose-to-braves-on-another-walk-off-single-trail-nlcs-2-0/'),\n",
       " ('Photos: NLDS Game 5, Dodgers vs. Giants',\n",
       "  'https://www.dailynews.com/2021/10/14/photos-nlds-game-5-dodgers-vs-giants/'),\n",
       " ('Photos: Dodgers rip Giants in NLDS Game 4 at Dodger Stadium',\n",
       "  'https://www.dailynews.com/2021/10/12/photos-dodgers-rip-giants-in-nlds-game-4-at-dodger-stadium/'),\n",
       " ('NLDS: Dodgers on brink of elimination with 1-0 loss to Giants in Game 3',\n",
       "  'https://www.dailynews.com/2021/10/11/nlds-dodgers-on-brink-of-elimination-with-1-0-loss-to-giants-in-game-3/'),\n",
       " ('Dodgers edge Cardinals on Chris Taylor’s walk-off home run, face Giants in NLDS',\n",
       "  'https://www.dailynews.com/2021/10/06/dodgers-edge-cardinals-on-chris-taylors-walk-off-home-run-face-giants-in-nlds/'),\n",
       " ('Photos: Dodgers prepare for wild-card game against Cardinals',\n",
       "  'https://www.dailynews.com/2021/10/05/photos-dodgers-prepare-for-wildcard-game-against-the-cardinals/')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_news_of(\"dodgers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.[15 points] \n",
    "---\n",
    "For this question, we will be using requests library to request\n",
    "```api.weather.gov``` to get an airport's weather information. Depending on that, you have to classify whether it is safe to fly or not.  \n",
    "\n",
    "*How to decide if it is safe to fly or not?*  \n",
    "We rely on a simple heuristic for now that if the ```shortForecast``` field has the word cloudy(independent of the case ofcourse) for any of the next three hours, then it is not safe to fly.  \n",
    "\n",
    "Input: String(Airport name)  \n",
    "Output: Boolean(True if safe, False otherwise)  \n",
    "\n",
    "\n",
    "FAQs:  \n",
    "Q. What link should I use for the requests.get() function?  \n",
    "A. Link would look something like this -> https://api.weather.gov/points/ ```<Latitude value>,<Longitude value>```   \n",
    "An Example would be: https://api.weather.gov/points/39.7456,-97.0892\n",
    "\n",
    "Q. I got something after requesting, but, I am not sure, what it is.  \n",
    "A. You recieved a response object. You can call .json() to it and see what is there. (See ```json.dumps()```)  \n",
    "\n",
    "Q. How do I get information from this json?  \n",
    "A. For getting info from that, as taught in the class, you can simply index them by using keys that you want after assessing the json.  \n",
    "\n",
    "Q. Okay, I indexed the json using the keys and I get some list out of it. What is that?  \n",
    "A. That list is the forecast for the next hours. You'll need this information for making the decision that is required in the question. Also, what you are getting in the list is a dict. **Make sure you understand what you are dealing with at each point.**  \n",
    "\n",
    "---\n",
    "\n",
    "Grading Rubric: Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airport_info():\n",
    "    file = open('Airports.txt', encoding='utf8')\n",
    "    values = dict()\n",
    "    cnt = 0\n",
    "    for line in file:\n",
    "        airport_name, coordinates = line.split('\\t')[1], line.split('\\t')[3].split(',')\n",
    "        long, lat = float(coordinates[0][1:]), float(coordinates[1][1:-2])\n",
    "        values[airport_name] = (lat, long)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "def to_fly_or_not(airport_name):\n",
    "    value = get_airport_info()\n",
    "    url = \"https://api.weather.gov/points/\" + str(value[airport_name][0]) + \",\" + str(value[airport_name][1]) \n",
    "    \n",
    "    content = requests.get(url)\n",
    "    \n",
    "    file = content.json() \n",
    "    another_url = file[\"properties\"][\"forecastHourly\"]\n",
    "    another_content = requests.get(another_url)\n",
    "    another_file = another_content.json()\n",
    "    list_periods = another_file[\"properties\"][\"periods\"]\n",
    "    count_list = []\n",
    "    \n",
    "    for each in list_periods:\n",
    "        temp = each[\"shortForecast\"]\n",
    "        if \"cloudy\" in temp.lower():\n",
    "            count_list.append(False)\n",
    "    for i in range(len(count_list)-2):\n",
    "        if count_list[i] == False or count_list[i+1] == False or count_list[i+2] == False:\n",
    "            return False\n",
    "        else:\n",
    "            return True          \n",
    "        \n",
    "to_fly_or_not('Los Angeles County Sheriff\\'s Department Heliport')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Question.[5 points]\n",
    "---\n",
    "For Question 1, if you want to get headlines daily, there is a possibility that you will be getting the articles that you have already read.  You need to know that which article is new and which article is old. So, how would you solve this problem?\n",
    "\n",
    "You just have to type your solution and not code for it.(I mean you can if you want but you won't get any points for it :))\n",
    "\n",
    "DO NOT ask me about the question. I might give away the solution by mistake! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use dictionary to store data. The dictionary has key as the article title, and value as the time the article is published. Each time we get a new article, we check if the dictionary's keyset contains that title already. If not, it means we have not read the article, so we add it to the dictionary.\n",
    "Otherwise, we have read that article, so we compare the value (the time that article was published) with the time of the article we just got. By that, we are able to check for duplicate articles, as well as tell which one is old and which one is new if there is duplicate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
